# Redis核心技术与实战-基础篇

## 0-开篇

#### Redis简介

1. Redis可以用于缓存、数据库、分布式锁等。

2. Redis的坑集中在下面四方面：
   
   1. CPU使用		：数据结构的复杂度、跨CPU核的访问
   2. 内存使用		：主从同步和AOF的内存竞争
   3. 储存持久化	：SSD上做快照的性抖动
   4. 网络通信		：多实例时的异常网络丢包
   
3. Redis知识点可以总结为：两大维度，三大主线。

   ![image-20210521205405730](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210521205405730.png)

   "两大维度"是指系统维度 和应用维度，"三大主线"是高性能、高可靠和高可扩展。

   在遇到问题的时候，按照上面的两个维度来减少可以按图索骥，快速找到影响的这些问题的关键因素。

   在应用维度上，推荐按照“**应用场景驱动**”和“**典型案例驱动**”，一个是**面**的梳理，一个是**点**的掌握。

4. Redis问题画像图

   ![image-20210521205750000](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210521205750000.png)

   在遇到问题时按照上图的问题关联来定位问题，从而实现针对性解决。

   在实践或掌握新知识点，按照“问题-->主线-->技术点”的方式梳理，放到上图，不断丰富上图。

## 1-基本架构

#### SimpleKV

通过介绍SimpleKV来对键值型数据库有个整体的认知

1. 构建SimpleKV的时候，首先要考虑里面可以存什么数据，对数据可以做什么样的操作，也就是数据模型和操作接口。

2. 上面的内容是Redis能够被用于缓存、秒杀、分布式锁等场景的重要基础。

3. 可以储存什么数据？

   1. 对于键值型数据库，基本数据模型是key-value模型，如“hello”:"world"。
   2. 在SimpleKV中，key是String类型，value是基本数据类型。
   3. 需要注意，对于不同键值型数据库，支持的key差异不大，而value却差异很大。
   4. 因此不同键值型数据库，重点考虑其支持的value类型。
   5. 如Memcached支持的value仅为String，Redis则可以支持多种value类型，这也是为啥它能够在实际场景得到广泛应用的原因。

4. 可以对数据做什么操作？

   1. SimpleKV既然是一个简单的键值数据库，无外乎就是增删改查，主要支持3种基本操作：PUT、GET和DELETE
      + PUT：新写入或更新一个key-value对
      + GET：根据一个key读取对应的value
      + DELTE：根据一个key删除对应的value
   2. 除此之外实际业务中还会存在根据一段key的范围返回value的值的情况。
   3. **综上，PUT/GET/DELETE/SCAN是有个键值型数据库的基本操作集合**。

5. 上面提到了数据模型和操作接口的设计，接下来就是考虑：**键值对保存在内存还是外存？**

6. 内存的好处是快（百ns级），但是断电丢失；外存虽然可以避免数据丢失，但是速度慢（几ms级）。

7. 因此在设计的时候，要考虑主要的应用场景，如缓存场景下要求快速访问且允许丢失。

8. 总体来说，一个键值型数据库主要包含：**访问框架、索引模块、操作模块和存储模块**四部分。

   ![image-20210521212720665](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210521212720665.png)



#### 访问模式

1. 访问模式主要分为两种：**通过函数库调用的方式提供外部应用的调用** 或 **通过网络框架以Socket通信形式对外提供键值对操作。**
2. 第一种对应上图的libsimplekv.so，以动态链接库形式链接到自己的程序（RocksDB采用）；
3. 第二种对应上图的网络框架包含了SocketServer和协议解析（Memcache和Redis采用）；
4. 在通过网络框架的方式提供访问的时候，虽然可以扩大数据库的受用面，但是也会带来一些潜在问题。
5. 使用网络框架的方式要对操作的命令就行封装，还需要对命令按照协议进行解析；
6. 在处理网络连接、网络请求的解析，以及数据的存取，要考虑使用一个线程还是多个线程、还是多个进程进行交互。
7. 上面提到的就是I/O模型设计，不同的模型对键值型数据库的性能和可扩展会有不同的影响。

#### 如何定位键值对的位置

1. 当SimpleKV解析了用户的请求，知道了要进行的键值对操作，此时SimpleKV需要查找要操作的键值对是否存在（索引模块）；
2. **索引的作用就是让键值型数据库根据key找到对应的value的储存位置，进而进行操作。**
3. 索引的类型比较多，如哈希表、B+树、字典树等等，不同索引在性能 、空间占用、并发控制上具有不同的特性。
4. Memcached和Redis采用哈希表作为key-value索引，而RocksDB采用跳表作为key-value索引。

#### 不同操作的具体逻辑

1. SimpleKV的索引模块负责根据key找到指定value之后，需要进一步操作，这是使用操作模块介入。
2. SimpleKV中操作模块是吸纳了不同的操作：
   + GET/SCAN，根据value的位置返回value即可
   + PUT，需要为其分配内存空间
   + DELETE，删除键值对并释放内存空间
3. 上面的分配和释放内存需要SimpleKV的储存模块来完成。

#### 重启后快速提供服务

1. SimpleKV采用常用的内存分配器glibc的malloc和free，因此并不需要特别考虑内存空间管理问题。
2. 但glibc在处理不同长度的键值对的时候，容易产生内存碎片，因此要处理好分配器的问题。
3. 对于以内存储存为主的Redis，内存分配器的处理就显得尤为重要，它提供了多种选择。
4. 虽然SimpleKV采用内存保存数据，但为了能够是实现重启后快速恢复服务，添加持久化功能。
5. 简化操作，直接以文件形式将键值对数据通过调用本地文件系统接口保存大磁盘中。

#### SimpleKV与Redis

上面简单构造了一个键值型数据库SimpleKV，对于同为键值型数据库的Redis采用了类似设计，但是功能更加丰富。

![image-20210521215314854](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210521215314854.png)

从上图我们可以看出，Redis相对于SimpleKV的主要区别如下：

+ Reids主要通过网络访问框架来进行访问，而不是动态库，这也是对Redis可以作为一个基础性的网络服务进行访问，扩大了其应用范围。
+ Redis数据模型中的value类型很丰富，因此有跟多的接口类型，如面向链表、集合等。
+ Redis支持的持久化方式有两种：日志（AOF）和快照（RDB），两者具有不同优势。
+ SimpleKV是单机版，Redis支持高可靠集群和高可扩展集群。

## 2-数据结构

#### 概述

1. Redis底层数据结构一共有6种：简单动态字符串、双向链表、压缩列表、哈希表、跳表和整数数组。

2. 这几种底层数据结构与Redis的数据保存类型的对应关系如下：

   ![image-20210521220651124](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210521220651124.png)

   从上图可以看出，String的实现方式只有一种，即简单动态字符串。

   而List、Hash、SorteSet和Set这是四种数据结构都有两种底层实现，通常情况下称这四种类型为集合类型。

#### 键和值采用什么数据结构

上面知道了不同使用类型底层的数据结构并不一样，这些都是value的实现。那么键和值之间采用什么结果进行映射呢？

1. Redis使用一个哈希表保持所有键值对，来实现键到值的快速访问。

2. 一个哈希表实际上是一个数组，其中每个元素称为一个哈希桶，因此常说哈希表由多个哈希桶组成，每个桶保存了键值对数据。

3. 不管是String还是四种集合类型，桶中保存的元素都是指向其的内存地址，如下图：

   ![image-20210521221654864](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210521221654864.png)

   其中哈希桶的entry元素保存了key和value的指针，分别指向实际的键和值，这样，即使值是集合也能通过value的指针找到。

   因为这个表保存了所有的键值对，因此称它为全局哈希表。

   哈希表的最大的好处就是能够以O(1)时间复杂度快速查找元素，只需要计算键的哈希值。

   由于计算哈希值与数据规模无关，当写入了大量数据如果发现系统操作变慢，那就是“**哈希表冲突和rehash可能带来的阻塞**”。

#### 为什么哈希表操作变慢了

1. 当往哈希表写入大量数据，哈希冲突是不可避免的。Redis中解决哈希冲突的方式是采用**链式哈希**。

2. 链式哈希，即在一个哈希桶中的多个元素用一个链表保存起来，节点之间通过指针连接，如下图：

   ![image-20210521222539206](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210521222539206.png)

3. 采用链式冲突的方式还有一个问题，即随着数据量增大，桶中的链表可能会很长，而链表中采用指针逐一查找，性能会很多。

4. Redis通过rehash操作来避免桶中的链表过程二导致的效率太低的问题。

5. Redis默认使用两个全局哈希表：哈希表1和哈希表2。

6. 一开始，插入数据的时候，默认使用哈希表1，此时的哈希表2并没有分配空间。

7. 随着数据量的增大，Redis开始执行rehash，这个过程分为三步：

   1. 把哈希表2分配更大的空间，例如是哈希表1的两倍；
   2. 把哈希表1中的数据重新映射并拷贝到哈希表2中；
   3. 释放哈希表1的内存空间。

8. 这样可以实现哈希表1到哈希表2的角色互换。

9. 上面的过程是简化的过程，在第2步中，如果直接一次性把哈希表1中的数据迁移到哈希表，会操作Redis线程阻塞。

10. 为了避免数据迁移产生的线程阻塞，Redis采用**渐进式rehash**，简单来说可以分为两步；

    1. 在数据拷贝时，Redis仍然正常处理客户端请求，每处理一个请求，从哈希表1中的第一个索引开始，隧道把其中拿到所有entry拷贝到哈希表2

    2. 等处理下一个请求时，在顺带拷贝哈希表1的下一个索引的entry

       ![image-20210521224006996](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210521224006996.png)

11. 通过rehash的方式，巧妙把一次性大量数据拷贝开销分摊到多钱请求处理的过程中，避免了耗时操作。

#### 集合数据操作效率

对于String类型，找到其对应在哈希桶的位置之后便可以进行操作了；但是对于集合类型，除了第一步的找到其对应在哈希桶的位置，还需要到集合中进行增删改查。

影响集合的操作效率是多因素的。首先是集合底层使用的数据结构，如使用哈希实现的要比链表实现的查询效率要高。其次就是与操作本身具有关系，比如读写一个元素操作要比读写所有元素效率高。

#### 有哪些底层数据结构

1. 集合底层的数据结构主要有5种：整数数组、双向链表、哈希表、压缩列表和跳表。
2. 哈希表、数组、双向链表比较常见，重点关注压缩列表和跳表。

#### 压缩列表

1. **压缩列表**实际上类似一个数组，数组中每个元素都对应一个数据。

2. 和数组不同的是，压缩列表在表头中有三个字段zlbytes、zltail和zllen，分别是列表长度、列表尾偏移和列表entry数量。

3. 压缩列表在表尾部还有一个zlend，表示列表结束。

   ![image-20210521225007525](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210521225007525.png)

4. 在压缩列表中，如果查找第一或最后一个元素，可以通过表头的三个字段的长度直接定位，时间复杂度是O(1)。

5. 如果查找其他元素就只能逐个查询，此时时间复杂度是O(n)。

#### 跳表

1. 有序链表只能逐一查找元素，导致操作起来很慢，于是有了跳表。

2. **跳表是在链表的基础上，添加了多级索引，通过索引位置的几个跳转，实现数据的快速定位。**

   ![image-20210521225233365](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210521225233365.png)

3. 从上图中可以看出，同建立若干级别的索引，可以跳过一些没必要的节点，可以加快遍历的速度。

4. 在跳表中查找的过程就是在索引中跳来跳去，最后定位元素，正好符合了“跳”表的叫法。时间复杂度O(logN)。

下面是各种数据结构的数据复杂度

![image-20210521225510901](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210521225510901.png)

#### 不同操作的复杂度

集合的类型提供的操作类型很多，有单个读写集合元素的，如HGET、HSET，也有操作多个元素的，如SADD，还有对集合进行遍历操作的，如SMEMBERS。上面的各种操作中其复杂度也不同，可以通过以下口诀快速背诵：

+ 单元素操作是基础；
+ 范围操作非常耗时；
+ 统计操作通常高效；
+ 统计操作通常高效。

**“单元素操作是基础”，指每个集合类型对的那个数据实现的增删改查，这些操作的时间的操作复杂度有集合采用的数据结构决定。**

这里需要注意的是，集合操作支持同时对多个元素增删改查(命令后面接多个参数)，这些操作的操作复杂度有单个操作复杂度和元素个数决定，如O(1)到O(M)。

如：HSET/HSET/HDEL、SADD/SREM/SRADMEMBER

**“范围操作非常耗时”，指集合类型中的遍历操作，可以返回集合中的所有数据。这类的时间复杂度一般是O(N)，比较好使，尽量避免。**

如：SMEMBERS、SRANGE

**在Redis2.8，提供了SCAN系列操作，可以实现渐进式遍历，每次返回有限的数据，可以避免一次性返回太多数据而导致Redis阻塞。**

如：HSCAN、SSCAN、ZSCAN

**“统计操作通常高效”，指集合操作对集合的所有元素个数的记录，这类的时间复杂度只有O(1)，以为这些结构中有专门的个数统计。**

如：LLEN、SCARD

**“统计操作通常高效”，值某些数据结构的特殊记录，如压缩列表和双向链表都会在记录头记录表头和表尾的偏移量，操作这些元素的时候时间复杂度O(1)。**

如：LPOP、RPOP、LPUSH、RPUSH

## 3-高性能IO模型

#### 概述

我们都知道Redis是单线程的，这里的单线程指的是Redis的网络IO和键值对的读写是由一个线程来完成，这也是Redis对外提供键值储存服务的主要流程。但是Redis的其他功能，如持久化、异步删除、集群数据同步等，其实都是有额外的线程执行。

这里重点关注Redis单线程设计以及多路复用机制。

#### Redis为什么使用单线程

使用单线程的原因主要是多线程的开销问题

1. 多线程给我们的印象就是，可以提高系统吞吐率，获胜增加系统扩展性；
2. 诚然多线程的合理使用可以增加系统中处理请求操作的资源实体，从而提升系统的吞吐率；
3. 但在引入了多线程之后，如果没有良好的系统设计，将会带来一系列的问题；
4. 如引入多线程之后，系统中会存在共享资源，对共享资源的管理将会引入额外的资源，即并发访问控制问题；
5. 并发访问控制是多线程开发中的难题，需要仔细考虑锁的粒度，否则可能导致系统的吞吐率比单线程下的还要慢；
6. 综上，引入多线程之后需要有良好的系统设计支撑才能够实现高吞吐率，否则额外的并发控制以及上下文切换可能会导致比单线程更糟的吞吐率。

#### 单线程Redis为什么这么快

1. Redis使用单线程模型能够达到每秒十万级别的处理能力，是Redis多方面设计选择的综合结果。
2. 一方面，Redis的大部分操作在内存中完成，再加上结合高效的数据结构，是实现高性能的一个重要原因；
3. 另一方面，Redis采用多路复用机制，使其在网络IO操作中能并发处理大量的客户端连接，实现高吞吐率。

#### 基本IO模型与阻塞点

在SimpleKV中，处理一个Get请求需要如下的步骤：

1. 监听客户端的请求（bind/listen）;
2. 和客户端建立连接（accept）；
3. 从socket中读取请求（recv）；
4. 解析客户端发送的请求（parse）；
5. 根据请求类型读取键值数据（get） ；
6. 最后给客户端返回结果，即向socket写入数据（send）；

bind/listen ===> accept ===> recv  ===> parse ===> **get**  ===> send

上面的流程中，除了get，其他都属于网络IO处理，get属于键值数据操作。

![image-20210522102734076](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522102734076.png)

在这些网络IO中，有潜在的阻塞点，分别是**accept()和recv()**。当Redis监听了到一个客户端有连接请求，但一直为建立成功，就会阻塞在accept()函数中，导致其他线程无法与Redis连接。类似地，通过recv()从一个客户端读取数据时，如果数据一直未到达，Redis也会阻塞在recv()。

综上，如果采用阻塞式网络模型，Redis将无法实现高性能，因此实际上Redis使用的是非阻塞Socket网络模型。

#### 非阻塞网络IO

Socket网络模型的非阻塞主要体现在三个关键函数，必须要了解三个函数返回类型以及设置模式。

在Socket模型中，不同操作会返回不同的套接字类型。Socket方法会自动返回主动套接字，然后调用listen()方法，将主动套接字转化为监听套接字，此时，可以监听来自客户端的连接请求。最后，调用accept()方法接收达到的客户端连接，并返回已连接的套接字。

![image-20210522103727763](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522103727763.png)

针对**监听套接字**，可以设置非阻塞模式，当Redis调用accept()但一直未有请求大大，Redis可以返回处理其他操作，而不用一直等待。

虽然Redis可以不用继续等待，但还是需要有机制继续在**监听套接字**上等待后继连接请求，并在有请求的时候通知Redis。

针对**已连接套接字**，也可以设置非阻塞模式，Redis调用recv()后，如果已连接套接字数据一直没有数据到达，Redis可以返回处理其他操作。

同样地，也需要有机制继续监听该**已连接套接字**，并在有数据到达时通知Redis。

通过设置非阻塞模式，Redis线程不会像基本IO模型那样一直在阻塞点等待，也不会导致Redis无法处理新进来的请求。

#### 基于多路复用的高性能IO模型

Linux中提供了一个机制，可以实现一个线程处理多个IO流，就是鼎鼎有名的select/epoll机制。

简而言之，**在Redis只运行单线程的情况下，该机制允许内核中，同时存在多个监听套接字和已连接套接字。**

内核会一直监听这些套接字的连接请求和数据请求，一旦请求到达，就会交给Redis线程处理，可以实现一个Redis线程处理多个IO流的效果。

![image-20210522104757901](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522104757901.png)

上图是基于多路复用的Redis IO模型，图中FD表示多个套接字。Redis网络框架调用Linux通过的epoll机制，让内核监听这些套接字。此时，Redis线程不会阻塞在某个特点的监听或已经连接的套接字上，这样Redis可以同时处理多个客户端连接并处理去，从而提高并发性。

为了在请求到达时能够通知Redis线程，**select/poll提供了基于事件的回调机制，即针对不同事件的发生，调用相应的处理函数。**

当select/poll一旦监听到FD上有请求到达时，就会触发相应的事件，具体是将这些事件放到一个事件队列中，Redis单线程不断对事件队列不断处理。这样一来，Redis不需要轮询是否有实际的事件方式，而是不断处理队列中的事件即可，避免浪费CPU资源。同时，Redis在对时间处理的时候，会调用对一个对应的处理函数，实现基于事件的回调。因为Redis一直对事件队列在处理，因此能一直响应客户端请求，提高Redis响应性。

下面通过一个客户端情况，详细解析一下：

1. 一个客户端连接请求包含了连接请求和读数据请求，分别对应Accept事件和Read事件，Redis分别对两个事件注册accept和get回调函数；
2. 当Linux内核监听到有连接请求或读数据请求时，会触发Accept时间和Read事件，此时，内核会调用Redis响应的accept和get函数进行处理。
3. 上面的过程就行去医院看病。需要先分诊、测体温、等记等。如果这些流程都由医生来干，显然效率低下。
4. 因此医院设置专门的分诊台，处理诊断前的工作（Linux内核监听），之后才会交给对应的医生处理（Redis单线程处理）。

需要注意的是，不同操作系统实现了不同的多路复用机制，如Linux的select/epoll，FreeBSD的kqueue和Solaris的evport。

#### 总结

Redis单线程指的是对网络IO和数据读写操作使用一个线程，而采用单线程的一个核心原因是可以避免并发控制的问题。

单线程Redis也能够获得高性能跟多路复用IO模型有关，它避免了accept和send/recv()潜在的网络IO阻塞。

补充阅读：[Redis 和 I/O 多路复用](https://draveness.me/redis-io-multiplexing/)

## 4-AOF日志

Redis的持久化机制主要有两种机制，即AOF日志和RDB快照。

#### AOF日志是如何实现的？

提到日志，比较熟悉的是数据库的写前日志（Write Ahead Log），即在实际写数据之前，先把修改的数据记到日志文件中，以便于故障时进行恢复。

但是与数据库的WAL不同，Redis的AOF日志是写后日志（Append Only File），即Redis先执行命令，把数据写入内存，然后才记录日志，如下图：

![image-20210522113215459](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522113215459.png)

传统的数据库日志，如redo log记录的是修改后的数据；而AOF记录的是Redis收到每一条命令，这些命令以文本形式保存。

比如，Redis收到了「set testkey testvalue」命令后，记录的日志如下图：

![image-20210522113432049](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522113432049.png)

其中：

+ 「*3」表示当前命令有三部分，每部分以「$+数字」开头，后面紧跟具体的命令、键或值；
+ 「$+数字」中的数字表示的是命令、键或值的总字节数；
+ 「$ 3 set」表示这部分有3个字节，即「set」命令。

为了避免额外的检查开销，Redis先AOF中记录日志的时候，不会对命令语法进行检查，因此「先写日志再写数据」的方式可能会导致记录了错误的命令，这时Redis使用日志恢复数据就可能出错。而「写后日志」的方式让系统先执行命令，执行成功才会记录到系统日志，否则系统会直接向客户端保存。因此，Redis使用写后日志可以避免记录出现错误命令。

除此之外，AOF还有一个好处就是不会阻塞当前写的操作。

显然，但是通过上面的方式，AOF是存在风险的。

首先，如果执行完一个命令，还没有来得及记录日志就宕机了，那么整个命令和对应的数据就会有丢失的风险。如果此时Redis是用作缓存，还可以从后台数据库中读取恢复，但是如果是Redis直接作为数据库使用，因为命令没有记入日志，所以就无法进行恢复。

其次，AOF虽然避免了对当前命令的阻塞，但可能会给下一个操作带来阻塞的风险，也是因为AOF日志也是在主线程中执行，如果把日志文件写入磁盘是，磁盘压力大，就会导致写盘慢，进而导致后继的操作无法执行。

分析可以知道，上面两个风险都来自写盘的时机。

#### 三种写回策略

对于写盘的问题，AOF机制提供给了三个选择，可以在「appendfsync」中配置：

+ **appendfsync always**，同步写回，每个命令执行完，立即写回磁盘；
+ **appendfsync everysec**，每秒写回，每个命令执行完，只是将其放到AOF内存缓冲区，每隔一秒将缓冲区内容写回磁盘；
+ **appendfsync no**，操作系统控制写回，每个命令写完，把其放到AOF缓冲区，有OS决定何时将缓冲区内容写回磁盘。

针对「避免主线程阻塞」和「减少数据丢失」问题，三种写回策略都无法做到两全其美，主要原因如下：

+ **always** 可以做到数据基本不丢失，但是每个写盘都会有一个慢速的落盘操作，不可避免会影响主线程；
+ **no** 在写完缓冲区，就可以执行后继命令，但是落盘时机不在Redis，一旦宕机可能会丢失数据。
+ **everysec** 采用每秒写回，避免了 always 的性能开销，同时可以实现保证 1s 外的数据不丢失，可以认为是数据丢失与主线程性能的折中考虑。

![image-20210522121330095](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522121330095.png)

因此在实际使用中，可以根据系统到高性能和高可靠的要求，采用不同的写回策略。

需要注意的是，在选择不同的写回策略的时候，好需要考虑一个因素，那就是「AOF文件过大」的问题。

AOF是以文件的形式记录接收的所有写命令，随着命令越来越多，AOF文件越来越多，这是就要考虑上面「文件过大」问题：

1. 文件系统对文件本身大小有限制，无法保存过大的文件；
2. 文件太大，往里面追加命令效率也会变低；
3. 如果发生宕机，AOF中的命令需要重新执行以实现故障恢复，如果AOF文件太大，恢复过程就会很缓慢。

为了避免上述的问题，可以使用**AOF重写机制**。

#### 日志太大怎么办？

AOF重写机制就是在重写的时候，Redis根据数据库现状创建一个新的AOF文件。核心思想就是**将旧日志文件中的多条命令，在重写后的新日志中变成一条命令**。

因为AOF采用的追加方式，逐一记录接收到的写命令。当一个键值对被多条写命令反复修改时，AOF会记录对应的多条命令。但是在重写的时候，是根据这个值的最新状态，为它生成写命令，这样就可以把多条命令变为一条命令，实现空间的“压缩”。

![image-20210522122310157](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522122310157.png)

上图对list修改了6次，最终结果是「D,C,N」，因此就可以用记录「LPUSH u:list "N","C","D"」即可。如果对于更多的修改，可以节省更多的空间。

虽然AOF重写可以减少AOF日志文件的体积，但是重写的过程还是要把整个数据最新的操作日志写回磁盘，仍然非常耗时，因此需要考虑：重写会不会阻塞主线程？

#### AOF重写会阻塞吗？

与AOF日志主线程不同，重写过程是在后台bgrewriteaof来完成，这也是为了避免阻塞主线程，导致数据库性能下降。

重写的过程可以简述为“**一个拷贝，两处日志**”。

**一个拷贝**，指每次执行重写时，主线程fork出后台线程的bgrewriteaof子进程。此时fork会把主线程的内存拷贝一份给bgrewriteaof子进程，里面包含了数据库的最新数据。然后bgrewriteaof子进程就可以在不影响主线程的情况下，逐一把数据写成操作，计入重写日志。

两处日志，由于主线程阻塞，仍可以处理新的请求。此时如果有写操作，第一处日志就是正在使用的AOF日志，Redis会把这个操作写到它的缓冲区。这样一来即时宕机，这个AOF日志操作仍然是齐全的。第二处日志指的是新的AOF重写日志。这个操作也会被写到重写日志的缓冲区。这样重写日志也不会丢失最新操作。等到拷贝数据的所有操作记录重写完成后，重写日志记录的最新操作也会写入新的AOF文件，以保证数据库最新状态的记录。此时，可以使用新的AOF文件代替就的AOF文件。

![image-20210522160657165](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522160657165.png)

简而言之，每次AOF重写时，Redis会先执行一个内存拷贝，用于重写；然后使用两个日志保证重写过程中，新写入的数据不会丢失。而且，因为Redis采用额外的线程进行数据重写，所以，这个过程并不会阻塞主线程。

#### 总结

Redis主要用两种持久化方式分别是，AOF日志和RDB快照。

其中AOF采用的是追加操作命令的方式来进行日志记录，可以在发生故障的时候进行恢复，保证数据可靠性。

AOF日志提供了不同的写回策略，让使用者进行灵活的选择，分别是No、Everysec、Always三种，分别代表的含义是：

+ No，Redis 不刷回磁盘，只复制将数据写入到AOF日志缓冲区，有OS决定何时刷回，这种适用于高性能的场景；
+ Everysec，Redis 每次写数据的时候，不立即刷回磁盘，而是以间隔一秒的方式进行刷回，这种是对性能和可靠的折中考虑；
+ Always，Redis每次写完数据，立即刷盘，这种适用于高可靠的场景。

此外，为了避免AOF日志过大，Redis 提供了AOF重写机制。其核心思想是，将对同一记录操作的多条AOF日志进行“合并”，只保留最后一次修改。

AOF重写的过程由后台线程完成，避免对主线程的阻塞。重写的时候先把主线程的内存（Redis数据）进行拷贝，然后依照拷贝依次记录重写日志。如果重写过程中来了新的请求，主线程会处理，产生的新日志会同时写入主线程和子进程中的AOF缓存，这样就能保证AOF重写缓存的数据是正确的。##

#### 评论区补充

**问题1**：Redis采用fork子进程重写AOF文件时，潜在的阻塞风险包括：fork子进程 和 AOF重写过程中父进程产生写入的场景，下面依次介绍。

+ fork子进程，fork这个瞬间一定是会阻塞主线程的（注意，fork时并不会一次性拷贝所有内存数据给子进程，老师文章写的是拷贝所有内存数据给子进程，我个人认为是有歧义的），fork采用操作系统提供的写实复制(Copy On Write)机制，就是为了避免一次性拷贝大量内存数据给子进程造成的长时间阻塞问题，但fork子进程需要拷贝进程必要的数据结构，其中有一项就是拷贝内存页表（虚拟内存和物理内存的映射索引表），这个拷贝过程会消耗大量CPU资源，拷贝完成之前整个进程是会阻塞的，阻塞时间取决于整个实例的内存大小，实例越大，内存页表越大，fork阻塞时间越久。拷贝内存页表完成后，子进程与父进程指向相同的内存地址空间，也就是说此时虽然产生了子进程，但是并没有申请与父进程相同的内存大小。那什么时候父子进程才会真正内存分离呢？“写实复制”顾名思义，就是在写发生时，才真正拷贝内存真正的数据，这个过程中，父进程也可能会产生阻塞的风险，就是下面介绍的场景。

+ fork出的子进程指向与父进程相同的内存地址空间，此时子进程就可以执行AOF重写，把内存中的所有数据写入到AOF文件中。但是此时父进程依旧是会有流量写入的，如果父进程操作的是一个已经存在的key，那么这个时候父进程就会真正拷贝这个key对应的内存数据，申请新的内存空间，这样逐渐地，父子进程内存数据开始分离，父子进程逐渐拥有各自独立的内存空间。因为内存分配是以页为单位进行分配的，默认4k，如果父进程此时操作的是一个bigkey，重新申请大块内存耗时会变长，可能会产阻塞风险。另外，如果操作系统开启了内存大页机制(Huge Page，页面大小2M)，那么父进程申请内存时阻塞的概率将会大大提高，所以在Redis机器上需要关闭Huge Page机制。Redis每次fork生成RDB或AOF重写完成后，都可以在Redis log中看到父进程重新申请了多大的内存空间。

**问题2**：AOF重写不复用AOF本身的日志，一个原因是父子进程写同一个文件必然会产生竞争问题，控制竞争就意味着会影响父进程的性能。二是如果AOF重写过程中失败了，那么原本的AOF文件相当于被污染了，无法做恢复使用。所以Redis AOF重写一个新文件，重写失败的话，直接删除这个文件就好了，不会对原先的AOF文件产生影响。等重写完成之后，直接替换旧文件即可。

## 5-内存快照

#### 概述

AOF进行持久化的时候主要面临两个问题，一方面AOF日志如果在不使用Always写回策略的时候，存在数据丢失的风险；另一方面，由于AOF日志保存的是操作记录，每次在数据恢复时要重新执行操作，在大数据量环境下很慢。

Redis中的另一种持久化方式，内存快照，可以同时实现保证可靠性和快速恢复。所谓内存快照就像拍照一下，保存某个时刻的数据状态信息。

对于Redis 而言，内存快照就把某个时刻的内存状态通过文件的方式保存到磁盘中，这样即使放生宕机也能通过内存快照进行恢复，数据库可靠性得到保证。这个快照文件称为RDB文件，即Redis DataBase文件。

使用RDB时，我们需要关注两个问题：

+ 对那些数据进行快照；（关系到效率）
+ 做快照时，数据能否增删改；（关系Redis是否应该阻塞，能否同时正常处理请求）

#### 对那些内存数据进行快照？

Redis中的数据都在内存，为了提供所有数据的可靠性，执行**全量快照**。即内存中的所有数据都记录在磁盘中，这样的好处一次性记录了所有数据，在快照过程中，需要保证数据的状态正确，同时在全量快照写入的时候文件较大，写磁盘的花销也会变大。

Redis 提供了两个命令来生成RDB文件，分别是save和bgsave

+ save，在主线程中执行，阻塞主线程
+ bgsave，创建一个子进程，专门用于希尔RDB文件，避免主线程的阻塞，是Redis中RDB文件的默认生成方式。

显然在使用bgsave的时候，还需要考虑一个问题，那就是主线程写入的新数据该然后处理？

#### 快照时数据能够修改吗？

如果在快照期间，数据不能被修改，那么就会导致快照阶段Redis的所有写服务将无法访问，这回对业务造成巨大的影响。

同时需要明确，bgsave并不意味着可以避免写阻塞，bgsave只是意味着使用子线程来去吧RDB快照写回到磁盘，而主线程此时不会被阻塞，可以用来执行读请求。即**避免阻塞和正常处理写操作并不是一回事**。

为了避免暂停写操作，Redis借助操作系统提供的写时复制（Copy-On-Write），保证执行快照的同时可以进行写操作。

bgsave子进程是由主线程fork生成，可以共享主线程的所有内存数据，bgsave运行后开始读取主线程的内存数据，并把它们写入RDB文件。

如果主线程要对数据进行读操作，那么子进程和主线程互不影响；但是如果主线程要对数据进行写操作，那么这块数据就会被复制，bgsav把这个副本写入RDB文件中，由于这个过程中主线程和子进程操作的是不同文件，因此并不互斥。

![image-20210522173751972](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522173751972.png)

同时我们应该意识到，由于数据一致被修改，写完快照之后，会存在一部分数据没来得及记录到快照中。这时就需要考虑快照的间隔。

#### 可以每秒一个快照吗？

和AOF日志的写盘策略类似，RDB不同快照的间隔会影响其数据可读可靠性。

![image-20210522174145080](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522174145286.png)

单纯从上图分析，t的越小，即间隔越小，数据丢失的概率就越小，但需要控制其下限。因为如果太小，一直在写盘，甚至上一次的快照还没写完，下一次快照又来啦，导致不断堆积。另一个方面如果bgsave执行频繁，不断通过fork操作从主线程创建出来，而fork操作需要阻塞主线程，而且主线程内存越多，阻塞时间越长，最终会阻塞到主线程执行。

为实现高可靠与高性能，可以使用增量快照，即在一次全量快照的基础上，后继只对修改部分进行快照记录，这样可以避免每次全量快照的开销。

为了实现对修改部分进行记录，需要引入额外的数据结构，用来记录那些数据进行了修改。

![image-20210522174926544](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522174926544.png)

需要注意的是，这些记录修改的数据结构占用的开销还是比较大，有可能被修改的记录只有8字节，而存储修改的结果就有32字节。

综上，可以发现，尽管RDB可以实现数据的快速恢复，但是如果快照的频率没把握好，就会导致不同的问题。一方面，如果快照频率太高，会都按照磁盘压力太大；另一方面，如果快照频率太低，有可能会丢失较多数据。

为了解决上面的痛点，Redis4.0提出了「**混合使用AOF日志和内存快照的方法**」。简而言之就是快照以一定频率执行，快照之间使用AOF日志进行记录。在这种情况下，RDB快照不需要频繁执行，节省fork对主线程的影响，同时AOF日志只需要保存两次快照之间的数据，不会出现文件太大的情况，也可以避免重写开销。

![image-20210522175651441](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522175651441.png)

采用混合方式记录既可以显式RDB文件快速回复的好处，又能享受到AOF只记录操作命令的简单优势。

#### 总结

使用AOF日志记录对Redis进行持久化的时候，可能出现AOF日志过大的问题（日志重写占用其他开销），同时在故障恢复时，引用AOF使用的是命令记录，恢复过程中需要执行对应的命令，恢复较慢。而RDB基于内存快照，直接将内存结构映射成磁盘文件，可以实现快速恢复。在写入RDB的时候，会在主线程中fork一个子进程，这个子进程通过Copy-On-Write对RDB文件进行刷盘，保证不会阻塞主线程的读写操作。在使用RDB快照方式的时候，还需要通过额外的结构来记录不同快照之间的数据变化。这种方式还是存在缺点，就是额外的记录还是占用比较多的空间。在Redis4.0，提出来「混合使用AOF日志和内存快照的方法」，即快照以一定频率进行执行，快照之间通过AOF日志进行记录，这种中情况下，RDB既不用频繁地执行快照，AOF也不会很大，不需要进行AOF重写，做到了“鱼和熊掌兼得”。

#### 评论区补充

问题：2核CPU、4GB内存、500G磁盘，Redis实例占用2GB，写读比例为8:2，此时做RDB持久化，产生的风险主要在于 CPU资源 和 内存资源 这2方面：

+ 内存资源风险：Redis fork子进程做RDB持久化，由于写的比例为80%，那么在持久化过程中，“写实复制”会重新分配整个实例80%的内存副本，大约需要重新分配1.6GB内存空间，这样整个系统的内存使用接近饱和，如果此时父进程又有大量新key写入，很快机器内存就会被吃光，如果机器开启了Swap机制，那么Redis会有一部分数据被换到磁盘上，当Redis访问这部分在磁盘上的数据时，性能会急剧下降，已经达不到高性能的标准（可以理解为武功被废）。如果机器没有开启Swap，会直接触发OOM，父子进程会面临被系统kill掉的风险。

+ CPU资源风险：虽然子进程在做RDB持久化，但生成RDB快照过程会消耗大量的CPU资源，虽然Redis处理处理请求是单线程的，但Redis Server还有其他线程在后台工作，例如AOF每秒刷盘、异步关闭文件描述符这些操作。由于机器只有2核CPU，这也就意味着父进程占用了超过一半的CPU资源，此时子进程做RDB持久化，可能会产生CPU竞争，导致的结果就是父进程处理请求延迟增大，子进程生成RDB快照的时间也会变长，整个Redis Server性能下降。
+ 另外，可以再延伸一下，老师的问题没有提到Redis进程是否绑定了CPU，如果绑定了CPU，那么子进程会继承父进程的CPU亲和性属性，子进程必然会与父进程争夺同一个CPU资源，整个Redis Server的性能必然会受到影响！所以如果Redis需要开启定时RDB和AOF重写，进程一定不要绑定CPU。

## 6-数据同步   

#### 概述

前面提到，Redis的持久化方式有两种，AOF和RDB，分别通过回放日志和重新读入RDB文件的方式恢复数据。但即使有了这两种方法，只能保证单点环境的数据安全，并不能保证数据库的高可靠性。如果这个实力宕机，在恢复期间将服务处理新的数据请求。

前面提到了Redis具有高可靠性，其中包含了两层含义：一是数据尽量少丢失，二是访问尽量少中断。AOF和RDB只能保证前者，但后者则需要通过冗余副本的方式来实现，即将一份数据保存在多个实例上。这样即使一个实例出现了故障，需要一段时间才能恢复，其他实例也可以对外提供访问，不依影响业务使用。

Redis中通过主从库模式，包含智能数据副本的一致，主从库之间采用的是读写分离的方式。

+ 读操作：主库、从库都可以接收；
+ 写操作：实现从主库执行，之后从库从主库中同步。

![image-20210522193850971](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522193850971.png)

集中在主库写是方便控制数据的一致性，避免不必要的加锁，实例间协商等开销。

#### 主从库如何第一次同步？

启动了多个Redis实例之后，相互之间通过replicaof（Redis5.0之前使用slaveof）命令形成主从库的关系，之后会按照三个阶段来完成第一次同步：

![image-20210522194255959](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522194255959.png)

上面是主（172.16.19.3）与从（172.16.19.3）的第一次同步的流程：

1. 首先，从服务器通过命令`replicaof 172.16.19.3`申请与主服务器形成主从关系；
2. **第一阶段，从库和主库建立连接，从库告诉主库将要同步，主从库间就可以开始同步；**
   + 具体就是，从库向主库发送**psync(包含主库runID和复制进度offset)**命令，表示要进行数据同步，主库根据这个命令就行启动复制；
   + runID：每个Redis 实例的唯一标识，第一次连接时，引用不知道主库ID，设置为？
   + offset：-1表示第一次复制
   + 主库在收到psync命令后，会用FULLRESYNC响应命令带上两个参数：主库runID和主库目前的复制进度offset，返回给从库
   + 从库收到响应后，会记录下两个参数。（**FULLRESYNC响应表示第一次复制采用全量复制，这时，主库把当前所有数据复制给从库**）
3. **第二阶段，主库将所有数据库同步个从库。从库收到数据后，在本地完成数据加载；这个过程依赖RDB文件；**
   + 具体是，主库执行bgsave命令，生成RDB文件，接着将文件发给从库
   + 从库收到RDB文件后，先清空当前数据库，然后加载RDB文件，这是为了避免从库前面的数据影响新的数据
   + 主库将数据同步给从库的时候，主库不会阻塞，仍然可以正常处理请求。否则，Redis的访问就中断
   + 如果请求中的写操作没有记录到刚生产的RDB文件，为了保证主从库的一致性，主库会在内存中专门创建replication buffer；
   + replication buffer是用来记录RDB文件生产后收到的所有写操作。
4. **第三阶段，主库把第二阶段执行过程中新收到的命令，再发送个从库。**
   + 具体是，当主库RDB文件发送后，就会包此时的replication buffer中的修改操作发送给从库，从库再重新执行这些操作。

#### 主从级联分担全量复制时的主库压力

在主从库第一次数据同步的过程中，一次全量RDB复制，对于主库主要有两个耗时操作：**生成RDB文件和传输RDB文件**。

如果主库数量很多，而且都要和主库进行全量复制，那么主库忙于fork子进程生成RDB文件，进行数据全量同步。fork这个操作会阻塞主线程处理正常请求，从而影响主库的响应速度。此外，传输RDB文件也会占用主库的网络带宽，同样给主库带来资源使用的压力。

为了降低主从复制时，主一段的压力，可以使用“主 - 从 - 从”模式。

在“主 - 从 - 从”模式下，将主库生成RDB和传输RDB的压力以级联的方式转嫁到其他从库上。

从库在建立主从关系的时候，不是与主库建立关系，而是与其中之一的从库建立关系`replicaof 从库ID 6379`；这样在同步的时候，不再是和主库交换，而是以级联的方式进行写同步。

![image-20210522200820165](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522200820165.png)

通过上面的一些措施，主库顺利地将全量复制同步到各个从库中。接下来，主从库之间会保持长连接用来后继陆续收到命令操作在同步给主库，这个过程也称为**基于长连接的命令传递**，可以避免频繁建立连接的开销。

#### 主从库网络断了咋办？

上面提到使用**基于长连接的命令传递**可以避免频繁建立连接的开销，但是如果发生网络断连或阻塞，主从库如何再次同步呢？

在Redis2.8前，主从库命令传输出现闪断，从库会和主库会和从库重新进行全量连接，开销很大；

在Redis2.8开始，网络断之后，主从库采用增量复制方式继续同步。增量指只把断开时间内产生的数据进行同步。

增量同步通过repl_backlog_buffer缓冲区来实现。

主库把与从库断开阶段收到的写命令，写入replication_buffer，同时也会写入repl_backlog_buffer缓冲区。

**repl_backlog_buffer是一块环形缓冲区，主库会记录自己写的位置，从库会记录自己已经读到的位置。**

![image-20210522201903901](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522201903901.png)

+ 一开始，主从库的读写位置在一起，可以认为是其实位置；
+ 随着主库不断接收到新的写操作，它在写缓冲区中的写位置会逐步偏离起始位置；
+ 主库的偏移量是master_repl_offset；从库的偏移量是slave_repl_offset；
+ 随着不停读写，两个偏移量会不断增加，有点像龟兔赛跑的感觉；
+ 主从库连接恢复，从库发psync请求给主库，带上slave_repl_offset，主库判断自己的master_repl_offset，把这个两者之间的命令返回给从库；

![image-20210522202359277](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522202359277.png)

需要注意的是，repl_backlog_buffer是一个环形缓冲区，如果用完会覆盖以前的内容，类似于循环链表。

**如果从库的读取速度比较慢，可能会导致未读取完毕，有些操作就被主库新写覆盖，导致主从不一致。**

为了避免上述问题，需要合理设置**repl_backlog_size**参数来设置环形缓冲区的大小，计算公式如下：

**缓冲空间大小 = 主库写入命令速度 * 操作大小 - 主从库间为了传输命令速度 * 操作大小**

实际中，考虑到突发的请求压力，可以把空间扩大一倍左右，对于高并发环境，可以设置为计算值的4倍，或者采用切片方式 。

#### 总结

Redis主从复制主要有三种模式：全量复制、基于长连接的命令传递、增量复制。

+ **全量复制**非常耗时，往往用在第一次同步。为此我们应该注意，一个Redis实例数据库不能太大，控制在几个GB，这样可以减少RDB文件生成、传输和重新加载的开销。另外需要避免集中请求主库进行全量复制，这样会增到主库的压力，可以采用“主 - 从 - 从”模式。
+ **长连接复制**是主从库复制后的常规同步阶段。这个阶段主从库通过命令传播实现同步。
+ **增量复制**发生在长连接复制期间连接中断的情况。这时需要特别注意repl_backlog_size参数的配置，否则可能会导致数据因覆盖而被丢失。

#### 评论区补充

**主从全量同步使用RDB而不使用AOF的原因：**

+ RDB文件内容是经过压缩的二进制数据（不同数据类型数据做了针对性优化），文件很小。而AOF文件记录的是每一次写操作的命令，写操作越多文件会变得很大，其中还包括很多对同一个key的多次冗余操作。在主从全量数据同步时，传输RDB文件可以尽量降低对主库机器网络带宽的消耗，从库在加载RDB文件时，一是文件小，读取整个文件的速度会很快，二是因为RDB文件存储的都是二进制数据，从库直接按照RDB协议解析还原数据即可，速度会非常快，而AOF需要依次重放每个写命令，这个过程会经历冗长的处理逻辑，恢复速度相比RDB会慢得多，所以使用RDB进行主从全量同步的成本最低。

+ 假设要使用AOF做全量同步，意味着必须打开AOF功能，打开AOF就要选择文件刷盘的策略，选择不当会严重影响Redis性能。而RDB只有在需要定时备份和主从全量同步数据时才会触发生成一次快照。而在很多丢失数据不敏感的业务场景，其实是不需要开启AOF的。

**另外，需要指出老师文章的错误：“当主从库断连后，主库会把断连期间收到的写操作命令，写入 replication buffer，同时也会把这些操作命令也写入 repl_backlog_buffer 这个缓冲区。”**

+ 主从库连接都断开了，哪里来replication buffer呢？

+ 应该不是“主从库断连后”主库才把写操作写入repl_backlog_buffer，只要有从库存在，这个repl_backlog_buffer就会存在。主库的所有写命令除了传播给从库之外，都会在这个repl_backlog_buffer中记录一份，缓存起来，只有预先缓存了这些命令，当从库断连后，从库重新发送psync $master_runid $offset，主库才能通过$offset在repl_backlog_buffer中找到从库断开的位置，只发送$offset之后的增量数据给从库即可。

**有同学对repl_backlog_buffer和replication buffer理解比较混淆，我大概解释一下：**

+ repl_backlog_buffer：就是上面我解释到的，它是为了从库断开之后，如何找到主从差异数据而设计的环形缓冲区，从而避免全量同步带来的性能开销。如果从库断开时间太久，repl_backlog_buffer环形缓冲区被主库的写命令覆盖了，那么从库连上主库后只能乖乖地进行一次全量同步，所以repl_backlog_buffer配置尽量大一些，可以降低主从断开后全量同步的概率。而在repl_backlog_buffer中找主从差异的数据后，如何发给从库呢？这就用到了replication buffer。

+ replication buffer：Redis和客户端通信也好，和从库通信也好，Redis都需要给分配一个 内存buffer进行数据交互，客户端是一个client，从库也是一个client，我们每个client连上Redis后，Redis都会分配一个client buffer，所有数据交互都是通过这个buffer进行的：Redis先把数据写到这个buffer中，然后再把buffer中的数据发到client socket中再通过网络发送出去，这样就完成了数据交互。所以主从在增量同步时，从库作为一个client，也会分配一个buffer，只不过这个buffer专门用来传播用户的写命令到从库，保证主从数据一致，我们通常把它叫做replication buffer。

+ 再延伸一下，既然有这个内存buffer存在，那么这个buffer有没有限制呢？如果主从在传播命令时，因为某些原因从库处理得非常慢，那么主库上的这个buffer就会持续增长，消耗大量的内存资源，甚至OOM。所以Redis提供了client-output-buffer-limit参数限制这个buffer的大小，如果超过限制，主库会强制断开这个client的连接，也就是说从库处理慢导致主库内存buffer的积压达到限制后，主库会强制断开从库的连接，此时主从复制会中断，中断后如果从库再次发起复制请求，那么此时可能会导致恶性循环，引发复制风暴，这种情况需要格外注意。

## 7-哨兵机制

#### 概述

上面提到了主从集群模式，这个模式下，如果从库发生了故障，客户端可以继续先主库或其他从库发送请求，进行相关的操作；但如果主库发生了故障，会直接影响从库的同步以及对Redis服务的写操作。

![image-20210522211101018](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522211101018.png)

显然，无论是服务中断还是Redis访问写操作，都是不允许的， 因此需要保证主库挂了，能够重新找到一个新的主库，比如把从库切换为主库。在这个过程中，需要明确三个问题：

1. 主库针对挂了吗？
2. 该选择哪个从库作为主库？
3. 怎么把新主库的相关信息通知给从库和客户端？

显然，Redis具有对应的解决办法，那就是哨兵机制。它可以实现在主从模式下，主从库自动切换。

#### 哨兵机制的基本流程

哨兵机制是一个运行在特殊模式下的Redis进程，主从库实例运行的同时，它也在运行。

哨兵主要负责三个任务：监控、选主和通知。

**监控。**监控是指哨兵进程在运行时周期性发送PING命令，检查主从库是否存活。如果从库没有在规定时间响应，会被标记为”**下线状态**“；如果主库没有在规定时间响应，哨兵会判断是否主库下线，下线则**自动切换主库**。

**选主。**主库挂了，哨兵就需要从多个从库中挑选出新的主库。这一步完成就有新的主库。

**通知。**执行完通知，哨兵把新主库连接信息发送到其他主库，让它们执行replicaof命令和新主库连接，并进行数据复制。同时哨兵把新的主库信息发送给客户端。

![image-20210522212546435](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522212546435.png)

#### 主观下线和客观下线

主库下线的判断是严谨的，哨兵对主库的下线分“主观下线”和“客观下线”两种。

哨兵通过PING命令检查自己与主从库的网络连接情况，用来判断实例的状态。

如果发现主库或从库对PING命令响应超时，哨兵会标记为“主观下线”。

检查的是从库时，由于从库对系统影响不大，因此可以简单地标记为“主观下线”；

对于主库，由于主库的切换对于系统影响很大，因此往往采用哨兵集群来判断主库是否真实下线（可能网络压力大）来避免误判。

只有哨兵集群大多数哨兵（大于等于N/2+1）认为主库下线，主库才会标记为“客观下线”。

![image-20210522213342631](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522213342631.png)

#### 如何选定新主库？

主库给“客观下线”之后，自然就需要选定新的主库，哨兵选择新主库的过程主要是“筛选+打分”。简而言之，在多个从库中，按照一定筛选条件，把不符合的去掉，在按照一定规则，给剩余的从库逐一打分。

![image-20210522213536126](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522213536126.png)

**首先是一定的筛选条件。**要保证选出的从库在线且状态较好，因此哨兵要对从库的在线状态以及网络连接情况就行判断，只有两者都通过才允许进入候选人，其他则需要被筛选掉。

**接下来是打分。**被筛选出来的主从库还需要按照三个规则(**从库优先级、从库复制进度从库ID号**)经历三轮打分，选出最终的主库。

**第一轮：优先级最高的从库得分高**

通过判断用户设置的slave-priority配置项。如果都一样进行第二轮打分。

**第二轮：和旧主库同步程度高的从库得分高**

通过判断从库中的salve_repl_offset字段判断与旧主库的同步程度，最接近master_repl_offset得分高。如果都一样进入第三轮

**第三轮：ID号小的从库得分高**

顾名思义。

#### 总结

哨兵机制是Redis保证服务不间断的重要保证。哨兵机制可以保证主从机器数据同步的顺利实现，是数据可靠的基础保证；哨兵机制的自动主从库切换时服务不间断的关键支持。

哨兵机制主要实现以下功能：

+ 监控主库运行状态，判断主库是否客观下线；
+ 在主库客观下线，选择新的主库；
+ 选择新的主库后，通知从库和客户端。

由于主库在Redis集群中的地位很重要，因此主库的“客观下线”需要哨兵间以“少数服从多数”的原则进行判断。

#### 评论区补充

**哨兵在操作主从切换的过程中，客户端能否正常地进行请求操作？**

+ 如果客户端使用了读写分离，那么读请求可以在从库上正常执行，不会受到影响。但是由于此时主库已经挂了，而且哨兵还没有选出新的主库，所以在这期间写请求会失败，失败持续的时间 = 哨兵切换主从的时间 + 客户端感知到新主库 的时间。

+ 如果不想让业务感知到异常，客户端只能把写失败的请求先缓存起来或写入消息队列中间件中，等哨兵切换完主从后，再把这些写请求发给新的主库，但这种场景只适合对写入请求返回值不敏感的业务，而且还需要业务层做适配，另外主从切换时间过长，也会导致客户端或消息队列中间件缓存写请求过多，切换完成之后重放这些请求的时间变长。

+ 哨兵检测主库多久没有响应就提升从库为新的主库，这个时间是可以配置的（down-after-milliseconds参数）。配置的时间越短，哨兵越敏感，哨兵集群认为主库在短时间内连不上就会发起主从切换，这种配置很可能因为网络拥塞但主库正常而发生不必要的切换，当然，当主库真正故障时，因为切换得及时，对业务的影响最小。如果配置的时间比较长，哨兵越保守，这种情况可以减少哨兵误判的概率，但是主库故障发生时，业务写失败的时间也会比较久，缓存写请求数据量越多。

**应用程序不感知服务的中断，还需要哨兵和客户端做些什么？**

+ 当哨兵完成主从切换后，客户端需要及时感知到主库发生了变更，然后把缓存的写请求写入到新库中，保证后续写请求不会再受到影响，具体做法如下：
  + 哨兵提升一个从库为新主库后，哨兵会把新主库的地址写入自己实例的pubsub（switch-master）中。客户端需要订阅这个pubsub，当这个pubsub有数据时，客户端就能感知到主库发生变更，同时可以拿到最新的主库地址，然后把写请求写到这个新主库即可，这种机制属于哨兵主动通知客户端。
  + 如果客户端因为某些原因错过了哨兵的通知，或者哨兵通知后客户端处理失败了，安全起见，客户端也需要支持主动去获取最新主从的地址进行访问。

+ 所以，客户端需要访问主从库时，不能直接写死主从库的地址了，而是需要从哨兵集群中获取最新的地址（sentinel get-master-addr-by-name命令），这样当实例异常时，哨兵切换后或者客户端断开重连，都可以从哨兵集群中拿到最新的实例地址。

+ 一般Redis的SDK都提供了通过哨兵拿到实例地址，再访问实例的方式，我们直接使用即可，不需要自己实现这些逻辑。当然，对于只有主从实例的情况，客户端需要和哨兵配合使用，而在分片集群模式下，这些逻辑都可以做在proxy层，这样客户端也不需要关心这些逻辑了，Codis就是这么做的。

**1、哨兵集群中有实例挂了，怎么办，会影响主库状态判断和选主吗？**

+ 这个属于分布式系统领域的问题了，指的是在分布式系统中，如果存在故障节点，整个集群是否还可以提供服务？而且提供的服务是正确的？这是一个分布式系统容错问题，这方面最著名的就是分布式领域中的“拜占庭将军”问题了，“拜占庭将军问题”不仅解决了容错问题，还可以解决错误节点的问题，虽然比较复杂，但还是值得研究的，有兴趣的同学可以去了解下。

  简单说结论：存在故障节点时，只要集群中大多数节点状态正常，集群依旧可以对外提供服务。具体推导过程细节很多，大家去查前面的资料了解就好。

**2、哨兵集群多数实例达成共识，判断出主库“客观下线”后，由哪个实例来执行主从切换呢？**

+ 哨兵集群判断出主库“主观下线”后，会选出一个“哨兵领导者”，之后整个过程由它来完成主从切换。
+ 但是如何选出“哨兵领导者”？这个问题也是一个分布式系统中的问题，就是我们经常听说的共识算法，指的是集群中多个节点如何就一个问题达成共识。**共识算法有很多种，例如Paxos、Raft，这里哨兵集群采用的类似于Raft的共识算法。**
+ 简单来说就是每个哨兵设置一个随机超时时间，超时后每个哨兵会请求其他哨兵为自己投票，其他哨兵节点对收到的第一个请求进行投票确认，一轮投票下来后，首先达到多数选票的哨兵节点成为“哨兵领导者”，如果没有达到多数选票的哨兵节点，那么会重新选举，直到能够成功选出“哨兵领导者”。

## 8-哨兵集群

#### 概述

前面提到，在主从模式下，哨兵机制是保证服务不间断重要基础，可以实现主库异常时实现主从的切换，一方面可以保证Redis服务能够始终可以被写，另一方面可以保证主从复制的顺利执行。

与Redis的读写进程一行，哨兵进程也可能发生宕机，这时就需要使用哨兵集群来保证系统的可靠性。

哨兵部署配置为：`sentinel monitor  <master-name> <ip> <redis-port> <quorum>`，其中并没有配置其他哨兵的连接信息。

#### 基于pub/sub机制的哨兵集群组成

和主从库的关联不同，哨兵节点的相互发现通过Redis提供的pub/sub机制，即发布/订阅机制。

哨兵只要和主库建立连接，就可以在主库发布信息，比如发布它自己的连接信息。同时可以从主库上订阅信息，获取其他哨兵发布的地址信息。当多个哨兵在主库上发送自己信息的消息之后，其他哨兵节点就能够收到这些信息，知道彼此的IP地址和端口。

Redis中以频道对消息分类，不同频道，可以认为是不同的消息类表。**只有订阅了同一个频道的应用，才能通过发布的消息进行信息交换。**

在主从集群中，主库上有一个名为`__sentinel__:hello`的频道，不同哨兵通过它相互发现，实现相互通信。

![image-20210522225531034](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522225531034.png)

不同哨兵经过对主库的发布订阅，可以知道对方的网络地址，然后相互进行网络连接，最终形成哨兵系统。

除此之外，哨兵集群中的节点需要对所有的从节点进行连接建立，从而实现监听主从库的心跳。

#### 哨兵如何通过从库IP地址和端口？

如果要想监听从库的心跳，首先要知道从库的地址。哨兵会通过先主库发送INFO命令先主库询问从库列表，之后哨兵根据从库列表与每个从库进行连接。

![image-20210522230054152](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522230054152.png)

从上面知道，多个哨兵已经可以通过对主库的发布订阅实现哨兵集群，通过先主节点发送INFO命令获取从库的列表，进而对从库进行心跳监听。接下来还剩下客户端与哨兵的通信。

#### 基于pub/sub机制的客户端事件通知

哨兵本质上也是一个Redis实例，只是它并不服务请求操作，而只是完成监控、选主和通知任务。所以每个哨兵也提供pub/sub机制，客户端可以从哨兵机制订阅消息。哨兵提供的订阅频道很多，不同频道包含了主从库切换过程中的不同关键事件，主要如下：

![image-20210522230612649](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522230612649.png)

知道这些频道之后，客户端就可以订阅这些频道，进而监听到主从切换等消息。具体步骤如下：

客户端读取哨兵的配置文件，获取哨兵的地址和端口，与其建立连接。

之后可以通过客户端执行订阅命令，来获取不同的事件消息。

有了事件通知，客户端不仅可以在主从切换得到新主库的连接信息，还可以监控主从库的切换过程中的各个重要事件。这样，客户端就可以知道主从切换进行到哪一步，有助于了解切换精度。

#### 由哪个哨兵执行主从切换？

哨兵的主从切换和主库的“客观下线”判断过程类似，需要“投票”。先回顾Redis主从库的切换过程：

哨兵集群要判断主库“客观下线”，就需要一定数量的实例都认为该主库已经“主观下线”。

任何一个哨兵实例只要自身判断了主库“主观下线”，就会先其他实例发送`is-master-down-by-addr`命令，其他实例收到之后会根据自己与实例的连接情况，做出Y或N的响应。

![image-20210522231503591](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210522231503591.png)

一个哨兵获得仲裁所需的赞成票之后，就可以标记为“客观下线”，票数可以通过quorum配置。

此时，这个哨兵就可以再给其他哨兵发送命令，表示希望自己来执行主从切换，并让所有其他哨兵进行投票。这个投票的过程叫做“Leader选举”。

在投票的过程中，任何一个想成为Leader的哨兵，要满足两个条件：

+ 第一，拿到半数以上的赞成票；
+ 第二，拿到的票数同时大于等于哨兵配置文件中的quorum值；

下面是哨兵数为3，quorum数为2的选举过程：

![image-20210523115301906](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210523115301906.png)

1. 哨兵1判断主库“客观下线”，自己想成为Leader，先给自己一张赞成票，然后分别给哨兵2、哨兵3发送命令，表示要成为Leader。
2. 哨兵3判断主库“客观下线”，自己也想成为Leader，先给自己一张赞成，然后分别先哨兵1、哨兵2发送命令，表示要成为Leader。
3. 哨兵1收到哨兵3的Leader投票请求，当然不答应，回复N；哨兵2先收到哨兵3的Leader投票请求，回复Y。
4. 这时，哨兵1的Leader投票请求姗姗来迟，到达哨兵2，但是哨兵2已结投票给哨兵3了，因此回复N。
5. 最终哨兵1（1Y，1N），哨兵2（2Y）。哨兵2不仅拿到超过半数的票数，而且达到了quorum，因此直接成为Leader。
6. 如果由于各种原因，没有哨兵拿到半数以上的票数，就会进行重新一轮选举。

需要注意的是，如果哨兵集群只有两个实例，此时一个哨兵想要称为Leader，必须获得2票，而不是1票。此时如果有个哨兵挂了，那么集群将无法进行主从库切换，因此至少会配置3个哨兵节点。

【资料补充】

[redis sentinel与raft协议](https://wjqwsp.github.io/2018/08/03/redis-sentinel%E4%B8%8Eraft%E5%8D%8F%E8%AE%AE/)

[Redis Cluster Specification](https://redis.io/topics/cluster-spec)

#### 总结

为了能够实现主从切换，引入了哨兵机制；为了避免单节点哨兵故障无法进行主从切换，以及减少误判率，引入了哨兵集群；哨兵集群机制有需要采用一些机制来保证其正常运行。

+ 基于pub/sub机制的哨兵集群组成过程；
+ 基于INFO命令的从库列表，是哨兵到从库的连接；
+ 基于哨兵自身的pub/sub功能，可以实现客户端和哨兵之间的事件通知。

对于主从库的切换，并不能是随意一个哨兵都可以执行，否则就乱套。这需要哨兵集群在判断了主库“客观下线”之后，经过仲裁，选举出一个Leader出来，有它负责实际的主从切换，即由它来完成新主库的选择以及通知从库与客户端。



#### 评论区补充

**Redis 1主4从，5个哨兵，哨兵配置quorum为2，如果3个哨兵故障，当主库宕机时，哨兵能否判断主库“客观下线”？能否自动切换？**

经过实际测试，我的结论如下：

1、哨兵集群可以判定主库“主观下线”。由于quorum=2，所以当一个哨兵判断主库“主观下线”后，询问另外一个哨兵后也会得到同样的结果，2个哨兵都判定“主观下线”，达到了quorum的值，因此，哨兵集群可以判定主库为“客观下线”。

2、但哨兵不能完成主从切换。哨兵标记主库“客观下线后”，在选举“哨兵领导者”时，一个哨兵必须拿到超过多数的选票(5/2+1=3票)。但目前只有2个哨兵活着，无论怎么投票，一个哨兵最多只能拿到2票，永远无法达到多数选票的结果。

**但是投票选举过程的细节并不是大家认为的：每个哨兵各自1票，这个情况是不一定的。下面具体说一下：**

场景a：哨兵A先判定主库“主观下线”，然后马上询问哨兵B（注意，此时哨兵B只是被动接受询问，并没有去询问哨兵A，也就是它还没有进入判定“客观下线”的流程），哨兵B回复主库已“主观下线”，达到quorum=2后哨兵A此时可以判定主库“客观下线”。此时，哨兵A马上可以向其他哨兵发起成为“哨兵领导者”的投票，哨兵B收到投票请求后，由于自己还没有询问哨兵A进入判定“客观下线”的流程，所以哨兵B是可以给哨兵A投票确认的，这样哨兵A就已经拿到2票了。等稍后哨兵B也判定“主观下线”后想成为领导者时，因为它已经给别人投过票了，所以这一轮自己就不能再成为领导者了。

场景b：哨兵A和哨兵B同时判定主库“主观下线”，然后同时询问对方后都得到可以“客观下线”的结论，此时它们各自给自己投上1票后，然后向其他哨兵发起投票请求，但是因为各自都给自己投过票了，因此各自都拒绝了对方的投票请求，这样2个哨兵各自持有1票。

场景a是1个哨兵拿到2票，场景b是2个哨兵各自有1票，这2种情况都不满足大多数选票(3票)的结果，因此无法完成主从切换。

经过测试发现，场景b发生的概率非常小，只有2个哨兵同时进入判定“主观下线”的流程时才可以发生。我测试几次后发现，都是复现的场景a。

**哨兵实例是不是越多越好？**

并不是，我们也看到了，哨兵在判定“主观下线”和选举“哨兵领导者”时，都需要和其他节点进行通信，交换信息，哨兵实例越多，通信的次数也就越多，而且部署多个哨兵时，会分布在不同机器上，节点越多带来的机器故障风险也会越大，这些问题都会影响到哨兵的通信和选举，出问题时也就意味着选举时间会变长，切换主从的时间变久。

**调大down-after-milliseconds值，对减少误判是不是有好处？**

是有好处的，适当调大down-after-milliseconds值，当哨兵与主库之间网络存在短时波动时，可以降低误判的概率。但是调大down-after-milliseconds值也意味着主从切换的时间会变长，对业务的影响时间越久，我们需要根据实际场景进行权衡，设置合理的阈值。

## 9-切片集群

#### 概述 

在大数据量下(5000万键值对，每个键值对512B)，Redis在运行的过程中，发现响应有时会很慢，快到秒级。

经过分析发现，数据总量约为25GB（5000万*512B），在使用RDB方式进行持久化的时候，Redis会fork子进程来完成，fork的用时和Redis的数据量成正相关，而fork执行时会阻塞主线程，这就是为什么Redis会出现响应有时候会很慢的原因，Redis在执行RDB持久化，而Redis中的数据量太多，导致fork的过程太慢。

这时，就需要数据进行切片从而减少每个Redis中的数据量，降低fork过程的响应时间。

切片集群，也称为分片集群，指的是启动多个Redis实例组成一个集群，然后按照一定的规定，把收到的数据分成多份，每一份用一个Redis实例保存。如将上面的25GB的数据切分放到5个实例上：

![image-20210523201207348](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210523201207348.png)

通过切片的方式，将25G数据切分到5台实例，每台实例中只包含了5GB数据，这样一来就可以规避大数据量下fork太慢阻塞主线程处理请求的问题。

#### 如何保存更多的数据？

Redis应对数据量增大主要使用两种方案：纵向扩展（scale up）和横向扩展（scale out）。

+ **纵向扩展**：升级实例的硬件配置，如增加内存、磁盘、增强CPU等

+ 横向扩展：横向增加Redis实例的个数，将数据放到不同的实例。

![image-20210523201728889](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210523201728889.png)

**纵向扩展优点**：实施简单、直接。

**纵向扩展缺点**：RDB对数据进行持久化时，fork的时间与数据量正相关，fork的时候可能会阻塞主线程（如果需要持久化则不需要考虑这个问题）。还有一个问题，**纵向扩展会受到硬件和成本的限制**，如内存不可能无上限增加。

**横向扩展优点**：不用担心单个实例的硬件和成本限制，只需要增加Redis的实例个数即可，横向扩展往往用于数据规模较大的情况。

**横向扩展缺点**：切片集群是个分布式系统，分布式的管理非常复杂，而切片集群有两大问题：

+ 数据切片，在多个实例之间如何分布？
+ 数据如何确定想要访问的数据在哪个实例上？

#### 数据切片和实例的对应分布关系

实际上，切片集群是一种保存大量数据的通用机制，这个机制可以有不同的是实现方案。

在Redis3.0前，官方没有针对切片集群提供具体方案。Redis3.0开始，官方提供了名为Redis Cluster的方案，用于实现切片集群。Redis Cluster方案中规定了数据和实例的对应规则。

Redis Cluster方案采用哈希槽来处理数据和实例之间的映射关系。在Redis Cluster中，一个切片集群共有16384个哈希槽，这些槽类似于数据分区，每个键值都有他对应的key，被映射到一个哈希槽中。

具体映射过程分为两大步：

1. 先根据键值对的key，按照CRC16算法计算一个16bit的值；
2. 然后再用这个16bit值对16384取模，得到0~16384范围的模式，每个模数代表一个相应编号的哈希槽。

在部署Redis Cluster方案时，可以使cluster create命令创建集群，此时，Redis会自动把这些槽平均分布在集群实例上。例如，如果集群有N个实例，那么每个实例上的槽的个数为16384/N个。

除此之外，也可以使用cluster meet命令手动创建实例间连接，形成集群，再使用cluster addslots命令，指定每个实例上的哈希槽的个数。

为什么会有了第一种方式（cluster create）还需要下面的方式（cluster meet）呢？

第一种方式采用的是平均的方式来创建，而第二种方式可以通过可以指定不同集群槽的数量。显然第一种方式简单、但不够灵活，对于多个实例，如果配置不同不能做到按照比例来划分槽，而第二种方式则可以解决这个问题。

![image-20210524102502053](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210524102502053.png)

在上图中，引用实例3的性能较弱，因此分到槽的比例也较小。

```bash
## 假设3台主机，5个槽
redis-cli -h 172.16.19.3 -p 6379 cluster addslots 0,1
redis-cli -h 172.16.19.4 -p 6379 cluster addslots 2,3
redis-cli -h 172.16.19.5 -p 6379 cluster addslots 4
```

**需要注意：在手动分配哈希槽的时候，需要把16384个槽都分配完，否则Redis集群将无法正常工作。**

上面通过数据到哈希槽，哈希槽到实例的分配。但是如何知道访问的数据在哪个实例上？

#### 客户端如何定位数据？

显然，分片之后需要先计算哈希槽，再通过哈希槽知道对应的实例，再在实例中操作对应的键值信息。

实例在哈希槽分配之后，会把分配的信息发送给其他与之链接的实例，进行扩散，这样一来集群中所有节点都能获得其他节点槽的分配信息。

即实例之间相互连接之后，每个实例都有所有哈希槽的映射关系。这样客户端在和集群建立连接时，实例就可以把哈希槽的分配信息发送给客户端。客户端拿到哈希槽信息之后会将其信息缓存在本地。当客户端请求键值时，先计算键对应的槽（CRC16），再去找对应的实例即可。

然而，在集群系统中，实例和哈希槽的数量并不是一成不变的，最常见的变化如下：

+ 集群中，有实例的新增或删除，Redis需要重新分配哈希槽；
+ 为了负载均衡，Redis需要把哈希槽在所有的实例上重新分布一遍；

此时，实例之间还可以通过相互传递消息，获取最新的哈希槽分配信息，但是客户端是无法主动感知变化的，这就导致客户端缓存的哈希槽信息和最新分配的信息不一致。

为了解决这种问题，Redis Cluster方案提供了一种**重定向机制**，允许客户端给一个实例发送数据读写操作是，这个实例上并没有相应的数据，客户端要再给一个新实例发送操作命令。

这个新实例的访问地址可以通过MOVED命令就行获取。

```bash
GET hello:key
(error)MOVED 13320 172.16.19.5:6379
```

MOVED命令表示，客户端请求的槽13320，实际是在172.16.19.5这个实例上，这样客户端就可以去新的实例中找数据。

![image-20210524105015197](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210524105015197.png)

上图中，用于负载均衡，Slot2已结从实例2全部迁移到实例3。但是客户端仍然使用之前的缓存。此时客户端访问Slot2的内容时，此时根据重定向机制，会发送MOVED命令，根据槽获取到槽在新节点的位置，重新申请。

如果Slot2中数据量较大，可能会出现一部分Slot2的数据在实例2，一部分的数据在实例3中。此时客户端访问Slot2内容，就会得到一个ASK响应，表示这个槽中的数据正在迁移 。

```bash
x GET hello:key
(error) ASK 13320 172.16.19.5:6379
```

ASK命名表示，客户端请求的键所在槽位于13320，在172.16.19.5实例上，但是这个哈希槽正在迁移。此时，客户端需要先给172.16.19.5实例发送ASKING命令。这个命令的意思是，让这个实例运行执行客户端发送的命令。然后客户端再向这个实例发送GET命令，以读取数据。

![image-20210524105732597](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210524105732597.png)

上图中，Slot2的内容正在从实例2迁移到实例3。此时客户端访问Slot2的内容key2，由于Slot2正在迁移（key1和key2已结迁移过去，key3和key4尚为迁移），实例2会返回ASK，表示这个槽的数据正在迁移。

ASK命令表示两层含义：

+ 表明Slot数据还在迁移；
+ ASK命令把客户端请求数据的最新实例地址返回给客户端，此时客户端需要给实例3发送ASKING命令，然后再发送操作命令。

和MOVED命令不同，ASK命令不会更新客户端换的哈希槽分配信息。因此上图中如果客户端再次访问Slot2的内容，仍然需要向实例2发送一次请求。因此，ASK的作用是让客户的能给新实例发送一次请求。

#### 总结

可以采用横向扩展和纵向扩展的方式进行数据量扩容。

纵向扩展可以认为是对单点系统的增强，如增加内存、磁盘、增强CPU等。这种方式简单直接，但是容易单点系统中数据太多，性能变慢。

横向扩展可以认为是对数据进行分片，将总的数据分别保存在不同的机器中，从而降低单点压力，同时能够保存大量数据。

Redis中的切片集群可以进行横向扩展，即使用多实例，每个实例根据比例（如性能）划分一定的槽，数据通过间的哈希值映射到哈希槽，在通过哈希槽分散保存在不同的实例中。

由于切片集群中实例是在动态变化的，可能会发送切片的增减，又或者为了实现负载均衡，要对数据进行重写分配（可能某个节点的槽整体压力很大，可以考虑将压力大的槽分配到其他实例上），这就会导致哈希槽和实例的映射关系发生变化。

在上面的情况下，客户端发送请求时，就会收到命令执行保存信息。

+ 如果访问槽中的全部数据已经迁移到新的节点，实例就会返回MOVED命令，让客户端去新的实例查找数据，同时会更新本地缓存的信息。
+ 如果访问槽中的数据未完全迁移到新的节点，实例就会返回ASK命令，表明数据正在迁移。此时客户端需要根据ASK返回的地址，使用ASKING命令，在发送操作命令就行查找。
+ 需要注意的是，当返回ASK命令时，不会更新本地缓存，因此下次再次访问该槽的时候，仍然会向旧的实例查询。

切片集群方案是Redis3.0才提出，因此如果是3.0之前，可以使用这些切片集群方案：

+ 基于客户端分区的SharedJedis；
+ 基于代理的Codis、Twemproxy；

#### 评论区补充

**隔壁分布式数据库也讲到了分片，但是它里面提到现代的分布式数据库实现分片基本都是Range-based的，能够实现分片的动态调度，适合互联网的场景。那为什么Redis依旧要用Hash-based的设计方式呢？是为了更高并发的写入性能吗？**

作者回复: 如果是根据某个字段的取值范围进行range-based分片，有可能的一个问题是：某个range内的记录数量很多，这就会导致相应的数据分片比较大，一般也叫做数据倾斜。对这个数据分片的访问量也可能大，导致负载不均衡。

基于记录key进行哈希后再取模，好处是能把数据打得比较散，不太容易引起数据倾斜，还是为了访问时请求负载能在不同数据分片分布地均衡些，提高访问性能。

**Redis Cluster不采用把key直接映射到实例的方式，而采用哈希槽的方式原因：**

1、整个集群存储key的数量是无法预估的，key的数量非常多时，直接记录每个key对应的实例映射关系，这个映射表会非常庞大，这个映射表无论是存储在服务端还是客户端都占用了非常大的内存空间。

2、Redis Cluster采用无中心化的模式（无proxy，客户端与服务端直连），客户端在某个节点访问一个key，如果这个key不在这个节点上，这个节点需要有纠正客户端路由到正确节点的能力（MOVED响应），这就需要节点之间互相交换路由表，每个节点拥有整个集群完整的路由关系。如果存储的都是key与实例的对应关系，节点之间交换信息也会变得非常庞大，消耗过多的网络资源，而且就算交换完成，相当于每个节点都需要额外存储其他节点的路由表，内存占用过大造成资源浪费。

3、当集群在扩容、缩容、数据均衡时，节点之间会发生数据迁移，迁移时需要修改每个key的映射关系，维护成本高。

4、而在中间增加一层哈希槽，可以把数据和节点解耦，key通过Hash计算，只需要关心映射到了哪个哈希槽，然后再通过哈希槽和节点的映射表找到节点，相当于消耗了很少的CPU资源，不但让数据分布更均匀，还可以让这个映射表变得很小，利于客户端和服务端保存，节点之间交换信息时也变得轻量。

5、当集群在扩容、缩容、数据均衡时，节点之间的操作例如数据迁移，都以哈希槽为基本单位进行操作，简化了节点扩容、缩容的难度，便于集群的维护和管理。

**另外，我想补充一下Redis集群相关的知识，以及我的理解：**

Redis使用集群方案就是为了解决单个节点数据量大、写入量大产生的性能瓶颈的问题。多个节点组成一个集群，可以提高集群的性能和可靠性，但随之而来的就是集群的管理问题，最核心问题有2个：请求路由、数据迁移（扩容/缩容/数据平衡）。

1、请求路由：一般都是采用哈希槽的映射关系表找到指定节点，然后在这个节点上操作的方案。

Redis Cluster在每个节点记录完整的映射关系(便于纠正客户端的错误路由请求)，同时也发给客户端让客户端缓存一份，便于客户端直接找到指定节点，客户端与服务端配合完成数据的路由，这需要业务在使用Redis Cluster时，必须升级为集群版的SDK才支持客户端和服务端的协议交互。

其他Redis集群化方案例如Twemproxy、Codis都是中心化模式（增加Proxy层），客户端通过Proxy对整个集群进行操作，Proxy后面可以挂N多个Redis实例，Proxy层维护了路由的转发逻辑。操作Proxy就像是操作一个普通Redis一样，客户端也不需要更换SDK，而Redis Cluster是把这些路由逻辑做在了SDK中。当然，增加一层Proxy也会带来一定的性能损耗。

2、数据迁移：当集群节点不足以支撑业务需求时，就需要扩容节点，扩容就意味着节点之间的数据需要做迁移，而迁移过程中是否会影响到业务，这也是判定一个集群方案是否成熟的标准。

Twemproxy不支持在线扩容，它只解决了请求路由的问题，扩容时需要停机做数据重新分配。而Redis Cluster和Codis都做到了在线扩容（不影响业务或对业务的影响非常小），重点就是在数据迁移过程中，客户端对于正在迁移的key进行操作时，集群如何处理？还要保证响应正确的结果？

Redis Cluster和Codis都需要服务端和客户端/Proxy层互相配合，迁移过程中，服务端针对正在迁移的key，需要让客户端或Proxy去新节点访问（重定向），这个过程就是为了保证业务在访问这些key时依旧不受影响，而且可以得到正确的结果。由于重定向的存在，所以这个期间的访问延迟会变大。等迁移完成之后，Redis Cluster每个节点会更新路由映射表，同时也会让客户端感知到，更新客户端缓存。Codis会在Proxy层更新路由表，客户端在整个过程中无感知。

除了访问正确的节点之外，数据迁移过程中还需要解决异常情况（迁移超时、迁移失败）、性能问题（如何让数据迁移更快、bigkey如何处理），这个过程中的细节也很多。

Redis Cluster的数据迁移是同步的，迁移一个key会同时阻塞源节点和目标节点，迁移过程中会有性能问题。而Codis提供了异步迁移数据的方案，迁移速度更快，对性能影响最小，当然，实现方案也比较复杂。

## 10-基础篇答疑

#### 和Redis相比、SimpleKV还缺少什么？

答案1：

1. 数据结构:缺乏广泛的数据结构支持，比如支持范围查询的SkipList和Stream等数据结构。
2. 高可用:缺乏哨兵或者master-slave模式的高可用设计;
3. 横向扩展:缺乏集群和分片功能;
4. 内存安全性:缺乏内存过载时的key 淘汰算法的支持;
5. 内存利用率:没有充分对数据结构进行优化，提高内存利用率，例如使用压缩性的数据结构;
6. 功能扩展:需要具备后续功能的拓展;
7. 不具备事务性:无法保证多个操作的原子性。

答案2：

SimpleKV所缺少的有:丰富的数据类型、支持数据压缩、过期机制、数据淘汰策略、主从复制、集群化、高可用集群等，另外，还可以增加统计模块、通知模块、调试模块、元数据查询等辅助功能。

答案3：

![image-20210524113638166](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210524113638166.png)



#### 整数数组和压缩列表作为底层数据结构的优势是什么？

整数数组和压缩列表的设计，充分体现了Redis“又快又省”特点中的“省”，也就是节省内存空间。整数数组和压缩列表都是在内存中分配一块地址连续的空间，然后把集合中的元素一个接一个地放在这块空间内，非常紧凑。因为元素是挨个连续放置的，我们不用再通过额外的指针把元素串接起来，这就避免了额外指针带来的空间开销。
我画一张图，展示下这两个结构的内存布局。整数数组和压缩列表中的entry都是实际的集合元素，它们一个挨一个保存，非常节省内存空间。

![image-20210524113850165](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210524113850165.png)

Redis之所以采用不同的数据结构，是从性能和内存使用效率之间的平衡。

#### Redis 基本IO模型中还有哪些潜在的性能瓶颈？

在Redis基本IO模型中，主要是主线程在执行操作，任何耗时的此操作，如bigkey，全量返回等都是潜在的性能瓶颈。

#### AOF重写过程中有没有其他潜在的阻塞风险？

这里有两个风险。
风险一: Redis主线程fork创建bgrewriteaof子进程时，内核需要创建用于管理子进程的相关数据结构，这些数据结构在操作系统中通常叫作进程控制块(Process Control Block，简称为PCB)。内核要把主线程的PCB内容拷贝给子进程。这个创建和拷贝过程由内核执行是会阻塞主线程的。而且，在拷贝过程中，子进程要拷贝父进程的页表，这个过程的耗时和Redis实例的内存大小有关。如果Redis实例内存大，页表就会大，fork执行时间就会长这就会给主线程带来阻塞风险。
风险二: bgrewriteaof子进程会和主线程共享内存。当主线程收到新写或修改的操作时，主线程会申请新的内存空间，用来保存新写或修改的数据，如果操作的是 bigkey，也就是数据量大的集合类型数据，那么，主线程会因为申请大空间而面临阻塞风险。因为操作系统在分配内存空间时，有查找和锁的开销，这就会导致阻塞。

#### AOF重写为什么不共享使用AOF本身的日志？

如果都用AOF日志，主线程要写，bgrewriteaof子线程也要写，这两者会竞争文件系统的锁，会对Redis主线程的性能造成影响。

#### 情景题

问题:使用一个2核CPU、4GB内存、500GB 磁盘的云主机运行Redis，Redis数据库的数据量大小差不多是2GB。当时 Redis主要以修改操作为主，写读比例差不多在8:2左右，也就是说，如果有100个请求，80个请求执行的是修改操作。在这个场景下，用RDB做持久化有什么风险吗?

**内存不足的风险：**

Redis fork 一个 bgsave子进程进行RDB写入，如果主线程在接收到写操作，就会采用写时复制。写时复制要给写操作分配新的内存空间。题目中的比例为80%，那么持久化的过程中，为了保存80%写操作涉及的数据，写时复制机制会在实例内存中，为这些数据再重新分配内存，分配的内存量相当于整个实例数据量的80%，大约是1.6GB，这样一来，整个系统内存的使用量接近饱和。此时如果实例有大量的key写入或者key修改，云主机内存很快就会吃光。如果云主机开启了Swap机制，就会有一部分数据被换到磁盘，当访问磁盘的这部分数据时，性能会急剧下降。如果云主机没有开启Swap功能，直接触发OOM，整个Redis实例可能会面临被系统kill的风险。

**主线程和子进程竞争使用CPU的风险：**

生成RDB的子进程需要CPU核运行，主线程本身需要CPU核运行，而且，如果Redis还启用了后台线程，此时，主线程、子进程和后台线程都会竞争CPU资源。由于云主机只有2核CPU，这就会影响主线程处理请求的速度。

####  为什么主从库间复制不使用AOF复制？

1. RDB文件是二级制文件，无论是要把RDB写入磁盘，还是要通过网络传输RDB，IO效率都不记录和传输AOF的高。
2. 在从库端匮乏时，RDB的恢复效率要比AOF高。

#### 如果想要应用程序不感知服务的中断，还需要哨兵和客户端在做些什么？

一方面，客户端需要能缓存应用发来的写请求。只要不是同步写操作，写请求通常不会在应用程序的关键路径上，所以，客户端缓存写请求后，给应用程序返回一个确认即可。

另一方面，主从切换完成后，客户端要能和新主库重新建立连接，哨兵需要提供订阅频道，让客户端能够订阅新主库的信息。同时，客户端也需要能主动和哨兵通信，询问新主库的信息。

#### **情景题**

**问题1:5个哨兵实例的集群，quorum值设为2。在运行过程中，如果有3个哨兵实例者发生故障了，此时，Redis 主库如果有故障，还能正确地判断主库“客观下线”吗?如果可以的话，还能进行主从库自动切换吗?**

因为判定主库的“客观下线”依据是，认为主库“主观下线”的哨兵个数要大于或等于quorum值，现在还剩下2个哨兵实例，个数正好等于quorum，所以能够正常判断主库是否处于“客观下线”状态。如果一个哨兵想要执行主从切换，就获得半数以上的哨兵投票赞成，也就是至少需要3个哨兵线程。但如今只有两个哨兵，因此无法完成主从切换。

#### 情景题

**问题2:哨兵实例是不是越多越好呢?如果同时调大down-after-milliseconds值，对减少误判是不是也有好处?**

哨兵越多，误判率越低，但是在判断主库下线以及选举Leader时，实例需要拿到的赞成票数也越多，等到所有哨兵投完票的时间也可能增加，主从库切换的时间增加，客户端容易堆积较多的请求，可能会导致客户端请求溢出，从而造成请求丢失。如果业务层对Redis的操作时间有要求，就可能会因为新主库一直没有选定，新操作无非执行而发生超时报警。

调大down-after-milliseconds值后，会导致判断主库故障的灵敏度下降，进而影响Redis对业务的可用性。

#### 为什么不直接用一个表，把键值对和实例的对应关系记录下来？

如果使用表记录键值对和实例的对应关系，一旦键值对和实例的对应关系发生了改变，就要修改表。如果是单线程操作表，那么所有操作都要串行执行，性能慢；如果是多线程操作表，就涉及到加锁开销。此外如果数据量很大，使用表记录键值对和实例的关系时，需要的额外空间也增加。

#### rehash的触发时机以及渐进式执行机制

**1 、Redis什么时候执行rehash？**

Redis会使用装载因子来判断是否需要rehash。装载因子的计算方式是，哈希表中所有的entriy的个数除以哈希桶的个数。redis会根据装载因子的两种情况来触发rehash操作：

+ 装载因子>=1，同时，哈希表允许进行rehash；
+ 装载因子>=5。

对于第一种情况，装载因子等于1，同时假设所有键值对平均分布在哈希表的各个桶中，那么，此时哈希表可以不用链式哈希，因为一个哈希桶正好保存了一个键值对。

但是，如果此时有新数据写入，哈希表就要使用链式哈希了，这会对查询性能产生影响。在进行RDB生成和AOF重写的时候，哈希表的rehash是被禁止的，这是为避免对RDB和AOF重写造成影响。如果此时，Redis没有在生成RDB和AOF，那么就可以进行rehash。否则，在有数据写入时，哈希表就要开始使用查询较慢的链式哈希了。

在第二种情况，即装载因子大于等于5，就表明当前保存的数据量已经远远大于哈希桶的个数，哈希桶有大量的链式哈希存在，性能会受到严重影响，此时就需要里面做rehash。

综上，当装载因子<1，或者装载因子在1到5之间同时Redis正在生成AOF和RDB，都不允许进行rehash。

**2、采用渐进式hash时，如果实例暂时没有收到新的请求，是不是就不做rehash？**

不是，Redis会执行定时任务，定时任务包含了rehash操作。所谓定时任务，就是按照一定频率（100ms/次）执行的任务。

在rehash被触发后，即使没有收到新的请求，Redis也会定时执行一次rehash操作，而且每次执行时长不会超过1ms，以免对其他任务造成影响。

#### 主线程、子进程和后台线程的联系和区别

线程和进程：

进程一般是资源分配的单元，一个进程拥有自己的堆、栈、虚存空间（页表）、文件描述符等；而线程一般指CPU进行调度和执行的实体。

主线程和主进程：

Redis启动，本身是有个进程，会接收客户端发送的请求，并处理读写操作请求。并且，接收请求和处理请求操作是Redis的主要工作，Redis没有依赖其他线程，因此一般把完成整个主要任务的Redis进程，称为主进程或主线程。

在主线程中，还可以使用fork创建子进程，或使用pthread_create创建线程。

Redis中使用fork创建的子进程主要有：

+ 创建RDB的后台子进程，同时由它负责在主从同步是传输RDB给从库；
+ 通过无盘复制方式传输RDB的子进程；
+ bgrewriteaof子进程。



![image-20210524180941773](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210524180941773.png)

#### 写时复制的底层实现机制

Redis使用RDB方式进行持久化时，会使用写时复制机制。bgsave子进程相当于复制原始数据，而主线程仍然可以修改原来的数据。

对于Redis，主线程fork出bgsave线程之后，bgsave子进程实际上是复制了主线程的页表。这些页表中保存了执行bgsave命令是，主线程的所有数据块在内存中的网络地址。这样bgsave子进程生成RDB时，可以根据页表读取这些数据，在写入磁盘中，如果此时，主线程接收到了新写或修改操作，那么，主线程会使用写时复制机制。主线程在有写操作时，才会把这个新写或修改后的数据写入到一个新的物理地址中，并修改自己的页表映射。

![image-20210524182704489](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210524182704489.png)

bgsave子进程复制主线程的页表之后，假如主线程修改虚页7的数据，那么，主线程就需要重新分配一个物理页，然后把修改后的虚页7的数据写到物理页53上，而虚页7里原来的数据仍然保存在物理页33上。这是，虚页7到物理页33的映射关系，仍然保留在bgsave子进程中，使用bgsave子进程可以无误的把虚页7的原始数据写入RDB文件中。

#### replication buffer 和 repl_backlog_buffer区别

主从复制时，Redis会使用replication buffer和repl_backlog_buffer。其中replication buffer是主从库全量复制的时候，主库用于和从库连接的客户端的buffer，而repl_backlog_buffer是为了支持从库增量复制，主库上用于持续保存写操作的一块专用buffer。

Redis主从复制时，当主库要把全量复制期间的写命令发给从库的时候，主库会先创建一个客户端，用来连接从库，然后通过这个客户端，把写操作命令发给从库。在内存中，主库上的客户端就会对应一个buffer，这个buffer称为replication buffer。Redis 通过client_buffer配置项来控制这个buffer大小。主库会给每个从库建立一个客户端，所以replication buffer不是共享的，而是每个从库都有对应一个的客户端。

repl_backlog_buffer是一块专用的buffer，在Redis服务器启动后，开始一直接收写操作命令，这是所有从库共享的。主库和从库各自记录自己的复制进度，所以，不同的从库在进行恢复时，会把自己的复制进度（slave_repl_offset）发送个主库，主库就可以和它独立同步。

![image-20210524201200261](https://tobing-markdown.oss-cn-shenzhen.aliyuncs.com/image-20210524201200261.png)